<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-148953677-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-148953677-1');
</script>

## Welcome to Mohna Chakraborty's homepage
<span style="font-size:1.35em;">
I am a third year Ph.D. student at the Department of Computer Science at Iowa State University. I am working as a Research Assistant at the <a href="https://sites.google.com/iastate.edu/qili/publications"> Data Mining and Knowledge Lab</a>. My supervisor is <a href="https://sites.google.com/iastate.edu/qili/">Dr. Qi Li</a>. Also, I have worked as a Data Science intern at The Home Depot, Epsilon and as Data Analytics intern at Delaware North. My research interests are in the domain of information extraction using weak supervision, review analysis,
data mining, natural language processing, and machine learning. My work focuses on solving the problem of the scarcity of labeled textual data, developing approaches that can facilitate the annotation process with minimal human effort and be implemented in daily-use systems without expensive hardware promoting accessibility to everyone. Through my research, I have contributed several key methods in top conferences and workshops. I have six published works in top conferences like ACL' 2023, UAI' 2023, SIGKDD' 2022, ESEC/FSE'2021 and workshops like RANLP'2021.</span>


## Publications

<span style="font-size:1.35em;">
<b><font color="red">2023</font></b><br>
<b>Mohna Chakraborty</b> , Adithya Kulkarni, and Qi Li.
Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts,
<b>ACL, 2023</b> [<a 
href="https://aclanthology.org/2023.acl-long.313"><span style='color:#954F72'>paper</span></a>]
<br><br> 

<span style="font-size:1.35em;">
<b><font color="red">2022</font></b><br>
Adithya Kulkarni, <b>Mohna Chakraborty</b> and Qi Li.
Optimal Budget Allocation for Crowdsourcing Labels for Graphs,
<b>UAI, 2023</b> [<a 
href="https://proceedings.mlr.press/v216/kulkarni23a.html"><span style='color:#954F72'>paper</span></a>]
<br><br> 

<span style="font-size:1.35em;">
<b><font color="red">2022</font></b><br>
<b>Mohna Chakraborty</b> , Adithya Kulkarni, and Qi Li.
Open-Domain Aspect-Opinion Co-Mining with Double-Layer Span Extraction,
<b>SIGKDD, 2022</b> [<a 
href="https://doi.org/10.1145/3534678.3539386"><span style='color:#954F72'>paper</span></a>]
<br><br> 

<span style="font-size:1.35em;">
<b><font color="red">2021</font></b><br>
Richard D Jiles, <b>Mohna Chakraborty</b>.
[Re] Domain Generalization using Causal Matching,
<b>ML Reproducibility Challenge, 2021</b>: [<a 
href="https://openreview.net/forum?id=r43elaGmhCY"><span style='color:#954F72'>paper</span></a>]
<br><br>
Abhishek Kumar Mishra, <b>Mohna Chakraborty</b>. Does local pruning offer task-specific models to learn effectively?,
<b>Proceedings of the Student Research Workshop Associated with RANLP, 2021</b>: [<a
href="https://aclanthology.org/2021.ranlp-srw.17"><span style='color:#954F72'>paper</span></a>]
<br><br>
<b>Mohna Chakraborty</b>. Does reusing pre-trained NLP model propagate bugs?,
<b>ESEC/FSE, 2021</b>: [<a
href="https://doi.org/10.1145/3468264.3473494"><span style='color:#954F72'>paper</span></a>]
<br><br>


## Recent News!
<span style="font-size:1.35em;">
<br> <b>May '23</b>: Joined The Home Depot as a Data Science intern.<br>
<br> <b>May '23</b>: Our paper on "Optimal Budget Allocation for Crowdsourcing Labels for Graphs" has been accepted at UAI 2023.<br>
<br> <b>May '23</b>: Our paper on "Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts" has been accepted at ACL 2023.<br>
<br> <b>March '23</b>: Awarded 2nd position for 7th Annual Research Competition at Iowa State University.<br>
<b>Sep '22</b>: Selected to represent Iowa State University for the prestigious and competitive Grace Hopper Celebration.<br>
<br> <b>Aug '22</b>: Awarded Student Travel Award for SIGKDD 2022.<br>
<br> <b>July '22</b>: I served as Review member at HCOMP 2022.<br>
<br> <b>July '22</b>: I served as Review member at EMNLP 2022.<br>
<br> <b>May '22</b>: Our paper on "Open-Domain Aspect-Opinion Co-Mining with Double-Layer Span Extraction" has been accepted at SIGKDD 2022.<br>
<br> <b>May '22</b>: Joined Epsilon as a PhD intern.<br>
<br> <b>April '22</b>: Defended my research proficiency on Weakly Supervised Review Analysis Based on Task Correlation.<br>
<br> <b>March '22</b>: Awarded 1st position for 6th Annual Research Competition at Iowa State University.<br>
<br> <b>March '22</b>: Our paper on "[Re] Domain Generalization using Causal Matching" has been accepted at ML Reproducibility Challenge 2021 (Fall Edition).<br>
<br> <b>Dec '21</b>: Served as Review member at PAKDD 2021.<br>
<br> <b>Sep '21</b>: Our paper on "Does local pruning offer task-specific models to learn effectively?" has been accepted at RANLP 2021.<br>
<br> <b>Aug '21</b>: Present SRC paper "Does reusing pre-trained NLP model propagate bugs?", ESEC/FSE, 2021.<br>
<br> <b>June '21</b>: Our paper on "Does reusing pre-trained NLP model propagate bugs?" has been accepted at ESEC/FSE, SRC 2021.<br>
<br> <b>May '21</b>: Joined Epsilon as a PhD intern.<br>
<br> <b>Aug '20</b>: Joined Ph.D. program at the Department of Computer Science at Iowa State University.<br></span>


